---
phase: 09-ongoing-improvement
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/db/schema/llm-usage.ts
  - src/db/schema/index.ts
  - src/agents/base-agent.ts
  - src/agents/claude-trader.ts
  - src/agents/openai-compatible-utils.ts
  - src/services/llm-cost-tracker.ts
autonomous: true

must_haves:
  truths:
    - "Token usage is captured from every LLM API call"
    - "Token usage is stored in database with agent, round, model info"
    - "Cost can be estimated from stored token counts"
  artifacts:
    - path: "src/db/schema/llm-usage.ts"
      provides: "LLM usage table schema"
      contains: "pgTable.*llm_usage"
    - path: "src/services/llm-cost-tracker.ts"
      provides: "Cost calculation and storage service"
      exports: ["recordLlmUsage", "estimateCost"]
    - path: "src/agents/base-agent.ts"
      provides: "AgentTurn type with usage field"
      contains: "usage.*inputTokens"
  key_links:
    - from: "src/agents/claude-trader.ts"
      to: "AgentTurn.usage"
      via: "response.usage extraction"
      pattern: "response\\.usage"
    - from: "src/agents/openai-compatible-utils.ts"
      to: "AgentTurn.usage"
      via: "response.usage extraction"
      pattern: "response\\.usage"
    - from: "src/agents/base-agent.ts"
      to: "src/services/llm-cost-tracker.ts"
      via: "recordLlmUsage call in runAgentLoop"
      pattern: "recordLlmUsage"
---

<objective>
Add LLM token usage tracking to capture API costs from Claude, GPT, and Grok agents.

Purpose: Without cost tracking, we cannot answer "Are agents economically viable?" This is the critical missing piece for benchmark profitability analysis.

Output:
- `llm_usage` database table storing token counts per round/agent
- Modified agents that extract and report token usage
- Cost estimation service with model pricing
</objective>

<execution_context>
@/Users/patruff/.claude/get-shit-done/workflows/execute-plan.md
@/Users/patruff/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/09-ongoing-improvement/09-RESEARCH.md

# Key files to understand patterns
@src/db/schema/agent-decisions.ts
@src/db/schema/index.ts
@src/agents/base-agent.ts
@src/agents/claude-trader.ts
@src/agents/openai-compatible-utils.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LLM Usage Schema and Cost Tracker Service</name>
  <files>
    src/db/schema/llm-usage.ts
    src/db/schema/index.ts
    src/services/llm-cost-tracker.ts
  </files>
  <action>
1. Create `src/db/schema/llm-usage.ts`:
```typescript
import { pgTable, text, integer, numeric, timestamp } from "drizzle-orm/pg-core";

export const llmUsage = pgTable("llm_usage", {
  id: integer("id").primaryKey().generatedAlwaysAsIdentity(),
  roundId: text("round_id").notNull(),
  agentId: text("agent_id").notNull(),
  model: text("model").notNull(),
  inputTokens: integer("input_tokens").notNull(),
  outputTokens: integer("output_tokens").notNull(),
  totalTokens: integer("total_tokens").notNull(),
  estimatedCostUsd: numeric("estimated_cost_usd", { precision: 10, scale: 6 }),
  createdAt: timestamp("created_at").defaultNow().notNull(),
});
```

2. Add export to `src/db/schema/index.ts`:
```typescript
export { llmUsage } from "./llm-usage.ts";
```

3. Create `src/services/llm-cost-tracker.ts`:
```typescript
import { db } from "../db/index.ts";
import { llmUsage } from "../db/schema/index.ts";

// Model pricing per million tokens (as of Feb 2026)
const MODEL_PRICING: Record<string, { input: number; output: number }> = {
  "claude-opus-4-5-20251101": { input: 15, output: 75 },
  "gpt-5.2-mini": { input: 10, output: 30 },
  "gpt-4o-mini": { input: 0.15, output: 0.6 },
  "grok-3-beta": { input: 5, output: 15 },
  "grok-beta": { input: 5, output: 15 },
};

export function estimateCost(model: string, inputTokens: number, outputTokens: number): number {
  const pricing = MODEL_PRICING[model] ?? { input: 10, output: 30 };
  return (inputTokens * pricing.input + outputTokens * pricing.output) / 1_000_000;
}

export interface LlmUsageRecord {
  roundId: string;
  agentId: string;
  model: string;
  inputTokens: number;
  outputTokens: number;
}

export async function recordLlmUsage(record: LlmUsageRecord): Promise<void> {
  const totalTokens = record.inputTokens + record.outputTokens;
  const estimatedCostUsd = estimateCost(record.model, record.inputTokens, record.outputTokens);

  await db.insert(llmUsage).values({
    roundId: record.roundId,
    agentId: record.agentId,
    model: record.model,
    inputTokens: record.inputTokens,
    outputTokens: record.outputTokens,
    totalTokens,
    estimatedCostUsd: estimatedCostUsd.toFixed(6),
  });
}

export async function getAgentCosts(agentId: string): Promise<{ totalCost: number; totalTokens: number }> {
  const rows = await db.select().from(llmUsage).where(eq(llmUsage.agentId, agentId));
  return {
    totalCost: rows.reduce((sum, r) => sum + parseFloat(r.estimatedCostUsd ?? "0"), 0),
    totalTokens: rows.reduce((sum, r) => sum + r.totalTokens, 0),
  };
}

import { eq, sql } from "drizzle-orm";

export async function getTotalCosts(): Promise<{
  totalCost: number;
  totalTokens: number;
  byAgent: Array<{ agentId: string; cost: number; tokens: number }>;
}> {
  const rows = await db.select().from(llmUsage);

  const byAgent = new Map<string, { cost: number; tokens: number }>();
  let totalCost = 0;
  let totalTokens = 0;

  for (const row of rows) {
    const cost = parseFloat(row.estimatedCostUsd ?? "0");
    totalCost += cost;
    totalTokens += row.totalTokens;

    const existing = byAgent.get(row.agentId) ?? { cost: 0, tokens: 0 };
    byAgent.set(row.agentId, {
      cost: existing.cost + cost,
      tokens: existing.tokens + row.totalTokens,
    });
  }

  return {
    totalCost,
    totalTokens,
    byAgent: Array.from(byAgent.entries()).map(([agentId, data]) => ({
      agentId,
      cost: data.cost,
      tokens: data.tokens,
    })),
  };
}
```
  </action>
  <verify>
`npx tsc --noEmit` passes with 0 errors related to new files. Schema exports correctly from index.ts.
  </verify>
  <done>
- llm_usage table schema exists with roundId, agentId, model, inputTokens, outputTokens, totalTokens, estimatedCostUsd, createdAt
- llm-cost-tracker.ts exports recordLlmUsage, estimateCost, getAgentCosts, getTotalCosts
- Schema exported from index.ts
  </done>
</task>

<task type="auto">
  <name>Task 2: Add Usage Field to AgentTurn and Capture in Agents</name>
  <files>
    src/agents/base-agent.ts
    src/agents/claude-trader.ts
    src/agents/openai-compatible-utils.ts
  </files>
  <action>
1. Modify `src/agents/base-agent.ts`:

Add `usage` field to `AgentTurn` interface (around line 144):
```typescript
export interface AgentTurn {
  toolCalls: ToolCall[];
  textResponse: string | null;
  stopReason: "tool_use" | "end_turn" | "max_tokens";
  usage?: {
    inputTokens: number;
    outputTokens: number;
  };
}
```

Add `totalUsage` accumulator and import in `runAgentLoop()` (around line 277):
```typescript
import { recordLlmUsage } from "../services/llm-cost-tracker.ts";

// Inside runAgentLoop, after line 317 (const toolTrace):
let totalUsage = { inputTokens: 0, outputTokens: 0 };

// Inside the tool-calling loop, after each callWithTools (line 321):
if (agentTurn.usage) {
  totalUsage.inputTokens += agentTurn.usage.inputTokens;
  totalUsage.outputTokens += agentTurn.usage.outputTokens;
}

// Before returning decision (around line 366-367), record usage:
// After parsing decision, before return:
const roundId = `round_${Date.now()}`;
if (totalUsage.inputTokens > 0 || totalUsage.outputTokens > 0) {
  await recordLlmUsage({
    roundId,
    agentId: this.config.agentId,
    model: this.config.model,
    inputTokens: totalUsage.inputTokens,
    outputTokens: totalUsage.outputTokens,
  }).catch((err) => console.error(`[${this.config.name}] Failed to record LLM usage:`, err));
}
```

2. Modify `src/agents/claude-trader.ts`:

In `callWithTools()` (around line 142), extract usage from response:
```typescript
// After parsing response content, before return:
return {
  toolCalls,
  textResponse,
  stopReason,
  usage: response.usage ? {
    inputTokens: response.usage.input_tokens,
    outputTokens: response.usage.output_tokens,
  } : undefined,
};
```

3. Modify `src/agents/openai-compatible-utils.ts`:

In `parseOpenAIResponse()` (around line 85), extract usage:
```typescript
return {
  toolCalls,
  textResponse: msg.content ?? null,
  stopReason,
  usage: response.usage ? {
    inputTokens: response.usage.prompt_tokens,
    outputTokens: response.usage.completion_tokens,
  } : undefined,
};
```
  </action>
  <verify>
`npx tsc --noEmit` passes. Run a local test if possible to verify usage is captured (check console logs or DB).
  </verify>
  <done>
- AgentTurn interface has optional `usage` field with inputTokens and outputTokens
- claude-trader.ts extracts response.usage.input_tokens and output_tokens
- openai-compatible-utils.ts extracts response.usage.prompt_tokens and completion_tokens
- base-agent.ts accumulates usage across turns and calls recordLlmUsage before returning
  </done>
</task>

<task type="auto">
  <name>Task 3: Run Database Migration</name>
  <files>None (database operation)</files>
  <action>
Push the new schema to the database:
```bash
npx drizzle-kit push
```

If drizzle-kit is not available, the table will be created on first insert via Drizzle's auto-create behavior (if enabled), or manually create:
```sql
CREATE TABLE IF NOT EXISTS llm_usage (
  id SERIAL PRIMARY KEY,
  round_id TEXT NOT NULL,
  agent_id TEXT NOT NULL,
  model TEXT NOT NULL,
  input_tokens INTEGER NOT NULL,
  output_tokens INTEGER NOT NULL,
  total_tokens INTEGER NOT NULL,
  estimated_cost_usd NUMERIC(10, 6),
  created_at TIMESTAMP DEFAULT NOW() NOT NULL
);
```

Verify table exists by checking DB or running a quick insert test.
  </action>
  <verify>
`npx drizzle-kit push` succeeds OR table exists in database. `npx tsc --noEmit` still passes.
  </verify>
  <done>
- llm_usage table exists in database
- Schema is synchronized with codebase
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with 0 new errors
2. `llm_usage` table schema exists in src/db/schema/
3. `llm-cost-tracker.ts` service exports recordLlmUsage, estimateCost, getTotalCosts
4. AgentTurn interface includes `usage?: { inputTokens, outputTokens }`
5. claude-trader.ts extracts `response.usage` from Anthropic response
6. openai-compatible-utils.ts extracts `response.usage` from OpenAI response
7. base-agent.ts accumulates usage and records it via llm-cost-tracker
</verification>

<success_criteria>
- Token usage is extracted from every LLM API call (Claude, GPT, Grok)
- Usage is accumulated across multi-turn tool-calling loops
- Usage is stored in llm_usage table with cost estimates
- TypeScript compiles without errors
</success_criteria>

<output>
After completion, create `.planning/phases/09-ongoing-improvement/09-01-SUMMARY.md`
</output>
